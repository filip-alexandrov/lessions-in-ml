{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents(tagset='universal')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for sentence_tag_pairs in corpus: \n",
    "    tokens = []\n",
    "    target = []\n",
    "\n",
    "    for token, tag in sentence_tag_pairs: \n",
    "        tokens.append(token) \n",
    "        target.append(tag) \n",
    "\n",
    "    inputs.append(token) \n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to json format\n",
    "import json \n",
    "\n",
    "with open('data.json', 'w') as f: \n",
    "    for x, y in zip(inputs, targets): \n",
    "        j = {'inputs': x, 'targets': y} \n",
    "        s = json.dumps(j) \n",
    "        f.write(f\"{s} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files = \"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = data['train'].shuffle(seed = 42).select(range(20_000))\n",
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = small.train_test_split(seed=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map targets to ints \n",
    "target_set = set()\n",
    "for target in targets: \n",
    "    target_set = target_set.union(target) \n",
    "target_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = list(target_set) \n",
    "id2label = {k: v for k, v in enumerate(target_list)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# also try using bert \n",
    "checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 \n",
    "t = tokenizer(data['train'][idx]['inputs'], is_split_into_words=True) \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value of i indicates it is the i'th word \n",
    "# in the input sentence (counting form 0) \n",
    "t.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_targets(labels, word_ids): \n",
    "    aligned_labels = []\n",
    "    for word in word_ids: \n",
    "        if word is None: \n",
    "            # it's a token like [CLS]\n",
    "            label = -100 \n",
    "        else: \n",
    "            # it's a real word \n",
    "            label = label2id[labels[word]]\n",
    "\n",
    "        # add the label \n",
    "        aligned_labels.append(label)\n",
    "        \n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try our function \n",
    "labels = data['train'][idx]['targets'] \n",
    "word_ids = t.word_ids()\n",
    "aligned_targets = align_targets(labels, word_ids) \n",
    "aligned_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_labels = [id2label[i] if i >= 0 else None for i in aligned_targets] \n",
    "for x, y in zip(t.tokens(), aligned_labels): \n",
    "    print(f\"{x}\\t{y}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize both inputs and targets \n",
    "def tokenize_fn(batch): \n",
    "    # tokenize the input sequence first \n",
    "    # this populates input_ids, attention_mask, etc\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch['inputs'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels_batch = batch['targets'] # original targets\n",
    "    aligned_labels_batch = []\n",
    "    for i, labels in enumerate(labels_batch): \n",
    "        word_ids = tokenized_inputs.word_ids(i) \n",
    "        aligned_labels_batch.append(align_targets(labels, word_ids))\n",
    "    \n",
    "    # recall: the 'target' must be stored in key called 'labels'\n",
    "    tokenized_inputs['labels'] = aligned_labels_batch\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to remove these from model inputs - they are neither inputs nor targets\n",
    "data['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = data.map(\n",
    "    tokenize_fn, \n",
    "    batched = True, \n",
    "    remove_columns = data[\"train\"].column_names, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer = tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists): \n",
    "    flattened = [val for sublist in list_of_lists for val in sublist]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(logits_and_labels): \n",
    "    logits, labels = logits_and_labels\n",
    "    preds = np.argmax(logits, axis = -1) \n",
    "\n",
    "    # remove -100 from labels and predictions \n",
    "    labels_jagged = [[t for t in label if t != -100] for label in labels] \n",
    "\n",
    "    # do the same for predictions whenever true labels is -100 \n",
    "    preds_jagged = [[p for p, t in zip(ps, ts) if t != -100] for ps, ts in zip(preds, labels)]\n",
    "\n",
    "    # flatten labels and preds \n",
    "    labels_flat = flatten(labels_jagged) \n",
    "    preds_flat = flatten(preds_jagged) \n",
    "\n",
    "    acc = accuracy_score(labels_flat, preds_flat) \n",
    "    f1 = f1_score(labels_flat, preds_flat, average='macro')\n",
    "\n",
    "    return {\n",
    "        \"f1\" : f1, \n",
    "        \"accuracy\" : acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[-100, 0, 0, 1, 2, 1, -100]]\n",
    "logits = np.array([[\n",
    "    [0.8, 0.1, 0.1], \n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.8, 0.1, 0.1], \n",
    "    [0.1, 0.8, 0.1], \n",
    "    [0.1, 0.8, 0.1], \n",
    "    [0.1, 0.8, 0.1], \n",
    "    [0.1, 0.8, 0.1]\n",
    "]])\n",
    "\n",
    "compute_metrics((logits, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    id2label = id2label, \n",
    "    label2id = label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"distilbert-finetuned-ner\", \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\", \n",
    "    num_train_epochs=2, \n",
    ")\n",
    "\n",
    "from transformers import Trainer \n",
    "\n",
    "trainer = Trainer(model = model, args = training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['test'], data_collator=data_collator, compute_metrics=compute_metrics, tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('my_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "pipe = pipeline(\n",
    "    \"token-classification\", \n",
    "    model= \"my_saved_model\", \n",
    "    device=0 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Bill Gates was the CEO of Microsoft in Seattle, Washington\" \n",
    "pipe(s)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
