{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"].features['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for later\n",
    "label_names = data['train'].features['ner_tags'].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# also try using bert \n",
    "checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 \n",
    "t = tokenizer(data['train'][idx]['tokens'], is_split_into_words=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value of i indicates it is in the i'th word \n",
    "# in the input sentence (counting from 0)\n",
    "t.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O, B-PER, I-PER B-ORG I-ORG B-LOC I-LOC B-MISC I-MISC\n",
    "begin2inside = {\n",
    "    1: 2, \n",
    "    3: 4, \n",
    "    5: 6, \n",
    "    7: 8\n",
    "}\n",
    "\n",
    "def align_targets(labels, word_ids): \n",
    "    aligned_labels = []\n",
    "    last_word = None\n",
    "\n",
    "    for word in word_ids: \n",
    "        if word is None: \n",
    "            label = -100 # start token [CLS]\n",
    "        elif word != last_word: \n",
    "            label = labels[word] # new word\n",
    "        else: \n",
    "            label = labels[word] # it's the same word as before \n",
    "            \n",
    "            # change B-<tag> to I-<tag> if necessary\n",
    "            if label in begin2inside: \n",
    "                label = begin2inside[label]\n",
    "        \n",
    "        aligned_labels.append(label) # add label \n",
    "        last_word = word # update last word\n",
    "    \n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try function \n",
    "labels = data['train'][idx]['ner_tags']\n",
    "word_ids = t.word_ids()\n",
    "aligned_targets = align_targets(labels, word_ids) \n",
    "aligned_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_labels = [label_names[t] if t >= 0 else None for t in aligned_targets] \n",
    "for x, y in zip(t.tokens(), aligned_labels): \n",
    "    print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make up a fake input just to test it \n",
    "words = [\n",
    "    '[CLS]', \"Ger\", \"##man\", \"call\", \"to\", \"boycott\", \"Micro\", \"##soft\", \"[SEP]\"\n",
    "]\n",
    "word_ids = [None, 0, 0, 1, 2, 3, 4, 4, None]\n",
    "labels = [7, 0, 0, 0, 3]\n",
    "aligned_targets = align_targets(labels, word_ids) \n",
    "aligned_labels = [label_names[t] if t >= 0 else None for t in aligned_targets]\n",
    "for x, y in zip(words, aligned_labels): \n",
    "    print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize both inputs and targets\n",
    "def tokenize_fn(batch): \n",
    "    # tokenize the input sequence first \n",
    "    # this populates input_ids, attention_mask, etc\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch['tokens'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels_batch = batch['ner_tags'] # original targets\n",
    "    aligned_labels_batch = [] # aligned targets\n",
    "    \n",
    "    for i, labels in enumerate(labels_batch): \n",
    "        word_ids = tokenized_inputs.word_ids(i) \n",
    "        aligned_labels_batch.append(align_targets(labels, word_ids))\n",
    "\n",
    "    # recall: \"target\" must be stored in key called \"labels\"\n",
    "    tokenized_inputs['labels'] = aligned_labels_batch\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to remove these from model inputs = they are neither inputs nor targets\n",
    "data['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = data.map(tokenize_fn, batched= True, remove_columns = data['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenized_datasets['train'][i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric \n",
    "\n",
    "metric = load_metric('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it out \n",
    "metric.compute(\n",
    "    predictions = [0, 0, 0], \n",
    "    references = [0, 0, 1]\n",
    ")\n",
    "# errors, expects batches of sequences (list of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it out \n",
    "metric.compute(\n",
    "    predictions = [[0, 0, 0]], \n",
    "    references = [[0, 0, 1]]\n",
    ")\n",
    "# warnings, labels are not tags (must be strigs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(\n",
    "    predictions = [[\"A\", \"A\", \"A\"]], \n",
    "    references = [[\"A\", \"B\", \"A\"]]\n",
    ")\n",
    "# again warning, because no tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(\n",
    "    predictions = [[\"O\", \"O\", \"I-ORG\", \"B-MISC\"]], \n",
    "    references = [[\"O\", \"B-ORG\", \"I-ORG\", \"B-MISC\"]]\n",
    ")\n",
    "# special computations based on IOB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def compute_metrics(logits_and_labels): \n",
    "    logits, labels = logits_and_labels\n",
    "    preds = np.argmax(logits, axis = -1) \n",
    "\n",
    "    # remove -100 from labels and predictions\n",
    "    # and convert the label_ids to label names\n",
    "    str_labels = [\n",
    "        [label_names[t] for t in label if t != -100] for label in labels\n",
    "    ]\n",
    "\n",
    "    # to the same for predictions whenever true label is -100\n",
    "    str_preds = [\n",
    "        [label_names[p] for p, t in zip(pred, targ) if t != -100] \\\n",
    "        for pred, targ in zip(preds, labels)\n",
    "    ]\n",
    "\n",
    "    the_metrics = metric.compute(predictions= str_preds, references = str_labels) \n",
    "    return {\n",
    "        \"precision\": the_metrics[\"overall_precision\"], \n",
    "        \"recall\": the_metrics['overall_recall'], \n",
    "        'f1': the_metrics['overall_f1'], \n",
    "        'accuracy': the_metrics['overall_accuracy']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {k:v for k, v in enumerate(label_names)}\n",
    "label2id = {v:k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint, \n",
    "    id2label = id2label, \n",
    "    label2id = label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"distilbert-finetuned-ner\", \n",
    "    evaluation_strategy='epoch', \n",
    "    save_strategy='epoch', \n",
    "    learning_rate=2e-5, \n",
    "    num_train_epochs=3, \n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args = training_args, \n",
    "    train_dataset=tokenized_datasets['train'], \n",
    "    eval_dataset=tokenized_datasets['validation'], \n",
    "    data_collator=data_collator, \n",
    "    compute_metrics=compute_metrics, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('my_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\n",
    "    \"token-classification\", \n",
    "    model = 'my_saved_model', \n",
    "    aggregation_strategy='simple', \n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Bill Gates was the CEO of Microsoft in Seattle, Washington.\"\n",
    "ner(s)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
