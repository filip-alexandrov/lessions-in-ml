{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/faa/Library/Python/3.9/lib/python/site-packages (4.25.1)\n",
      "Requirement already satisfied: requests in /Users/faa/Library/Python/3.9/lib/python/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /Users/faa/Library/Python/3.9/lib/python/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/faa/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2022.12.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-20 22:08:54--  https://lazyprogrammer.me/course_files/AirlineTweets.csv\n",
      "Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.21.23.210, 172.67.213.166\n",
      "Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.21.23.210|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3421431 (3,3M) [text/csv]\n",
      "Saving to: ‘AirlineTweets.csv’\n",
      "\n",
      "AirlineTweets.csv   100%[===================>]   3,26M   645KB/s    in 5,4s    \n",
      "\n",
      "2023-04-20 22:09:01 (620 KB/s) - ‘AirlineTweets.csv’ saved [3421431/3421431]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://lazyprogrammer.me/course_files/AirlineTweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Basic usage \n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text_classification.TextClassificationPipeline"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998722076416016}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"this was a great movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mcurrent_device()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/cuda/__init__.py:674\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurrent_device\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    673\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     _lazy_init()\n\u001b[1;32m    675\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Use GPU \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m classifier \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39msentiment-analysis\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    868\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[0;32m--> 870\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline_class(model\u001b[39m=\u001b[39;49mmodel, framework\u001b[39m=\u001b[39;49mframework, task\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/text_classification.py:83\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 83\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_model_type(\n\u001b[1;32m     86\u001b[0m         TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n\u001b[1;32m     87\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         \u001b[39melse\u001b[39;00m MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n\u001b[1;32m     89\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:778\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, modelcard, framework, task, args_parser, device, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m# Special handling\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 778\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    780\u001b[0m \u001b[39m# Update config with task specific parameters\u001b[39;00m\n\u001b[1;32m    781\u001b[0m task_specific_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtask_specific_params\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:1682\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1678\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1679\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1680\u001b[0m     )\n\u001b[1;32m   1681\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1682\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Use GPU \n",
    "classifier = pipeline(\"sentiment-analysis\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv('./data/AirlineTweets.csv')\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_[['airline_sentiment', 'text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['airline_sentiment'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['airline_sentiment'] != 'neutral'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {'positive': 1, 'negative': 0} \n",
    "df['target'] = df['airline_sentiment'].map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  target\n",
       "1          positive  @VirginAmerica plus you've added commercials t...       1\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...       0\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...       0\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...       0\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX...       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11541"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m texts \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[39m=\u001b[39m classifier(texts)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    156\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1063\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1060\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1064\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1065\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:1063\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1060\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1064\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1065\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    989\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 990\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    991\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pipelines/text_classification.py:182\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/distilbert/modeling_distilbert.py:761\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    759\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 761\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    762\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    763\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    764\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    765\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    766\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    767\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    768\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    769\u001b[0m )\n\u001b[1;32m    770\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    771\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/distilbert/modeling_distilbert.py:579\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    580\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    581\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    582\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    583\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    584\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    585\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    586\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/distilbert/modeling_distilbert.py:354\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    352\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 354\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    355\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    357\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    359\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/distilbert/modeling_distilbert.py:289\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    290\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    291\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    292\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    293\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    294\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    295\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    296\u001b[0m )\n\u001b[1;32m    297\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    298\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/distilbert/modeling_distilbert.py:229\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    227\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(weights, v)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    228\u001b[0m context \u001b[39m=\u001b[39m unshape(context)  \u001b[39m# (bs, q_length, dim)\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_lin(context)  \u001b[39m# (bs, q_length, dim)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    232\u001b[0m     \u001b[39mreturn\u001b[39;00m (context, weights)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "texts = df['text'].tolist()\n",
    "predictions = classifier(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute probability for being positive\n",
    "probs = [d['score'] if d['label'].startswith(\"P\") else 1 - d['score'] for d in predictions]\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [1 if d['label'].startswith(\"P\") else 0 for d in predictions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('acc:', np.mean(df['target'] == preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df['target'], preds, normalize='true')\n",
    "cm\n",
    "# large numbers along diagonal is encouraging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn is transitioning to v1 \n",
    "def plot_cm(cm): \n",
    "    classes = ['negative', 'positive']\n",
    "    df_cm = pd.DataFrame(cm, index = classes, columns= classes) \n",
    "    ax = sn.heatmap(df_cm, annot=True, fmt = 'g')\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Target\")\n",
    "\n",
    "plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(df['target'], preds)\n",
    "# best for document retrieval, can be inaccurate in certain situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(1 - df['target'], 1 - preds) \n",
    "# inverted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(df['target'], probs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(1 - df['target'], 1 - np.array(probs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
